# src/paiza_crawler.py
import re
import time
from playwright.sync_api import sync_playwright
from ai_client import analyze_job_description
from notification import send_discord_notify
from profile import MY_PROFILE
import db

AUTH_FILE = "auth.json"
MAX_CHECK_LIMIT = 20 
SCORE_THRESHOLD = 70

def clean_text(text):
    return text.replace("\n", " ").strip()

def run_crawler():
    print("ğŸ¤– Paiza Direct Crawler (Final) èµ·å‹•")
    
    with sync_playwright() as p:
        # headless=True ã§ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‹ã‹ãªã„
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(storage_state=AUTH_FILE)

        page_list = context.new_page()   # [ã‚¿ãƒ–A] ä¸€è¦§ç”¨
        page_worker = context.new_page() # [ã‚¿ãƒ–B] è§£æç”¨

        try:
            # ãƒªã‚¹ãƒˆãŒç¢ºå®Ÿã«èª­ã¿è¾¼ã¾ã‚Œã‚‹URLã‚’æŒ‡å®š
            start_url = "https://paiza.jp/messages?from=golden_scout"
            print(f"ğŸš€ [List] {start_url} ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
            page_list.goto(start_url)
            
            # æ­£ã—ã„ãƒªã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒŠã‚’æŒ‡å®š
            sidebar_selector = ".p-messages-scout-messages"
            try:
                # ã¾ãšã‚³ãƒ³ãƒ†ãƒŠãŒå‡ºã‚‹ã®ã‚’å¾…ã¤
                page_list.wait_for_selector(sidebar_selector, state="visible", timeout=10000)
                
                # ã€Œã‚³ãƒ³ãƒ†ãƒŠã®ä¸­ã«ãƒªãƒ³ã‚¯(aã‚¿ã‚°)ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã€ã®ã‚’æ˜ç¤ºçš„ã«å¾…ã¤
                # ä¸­èº«ãŒç©ºã®ã†ã¡ã¯å…ˆã«é€²ã¾ã›ãªã„
                print("â³ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã®æç”»ã‚’å¾…æ©Ÿä¸­...")
                # ã‚³ãƒ³ãƒ†ãƒŠå†…ã®aã‚¿ã‚°ã‚’æ¢ã™ã‚»ãƒ¬ã‚¯ã‚¿
                list_item_selector = f"{sidebar_selector} a"
                page_list.wait_for_selector(list_item_selector, state="attached", timeout=10000)
                
                # å¿µã®ãŸã‚å°‘ã—å¾…æ©Ÿï¼ˆã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç­‰ã®å®Œäº†å¾…ã¡ï¼‰
                page_list.wait_for_timeout(2000)

            except Exception as e:
                print(f"âŒ ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return

            print("ğŸ•µï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
            sidebar = page_list.locator(sidebar_selector)
            
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦éå»åˆ†ã‚‚å°‘ã—èª­ã¿è¾¼ã‚€
            sidebar.hover()
            for _ in range(3):
                page_list.mouse.wheel(0, 1000)
                page_list.wait_for_timeout(500)
            
            # ä¸€ç•ªä¸Šã«æˆ»ã™
            sidebar.evaluate("el => el.scrollTop = 0")
            page_list.wait_for_timeout(1000)

            # ãƒªãƒ³ã‚¯å–å¾—
            message_links = sidebar.locator("a").all()
            print(f"ğŸ“‹ æ¤œå‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ç·æ•°: {len(message_links)} ä»¶")

            processed_count = 0
            
            # è§£æãƒ«ãƒ¼ãƒ—
            target_count = min(len(message_links), MAX_CHECK_LIMIT)
            
            for i in range(target_count):
                link = message_links[i]
                try:
                    title_text = clean_text(link.inner_text())[:30]
                    
                    # ç©ºã®ãƒªãƒ³ã‚¯ã‚„ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³ç­‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                    if not title_text or "ã‚‚ã£ã¨è¦‹ã‚‹" in title_text: continue
                    
                    # [ã‚¿ãƒ–A] ã‚¯ãƒªãƒƒã‚¯
                    # ãƒªã‚¹ãƒˆå†…ã®è¦ç´ ãŒéš ã‚Œã¦ã„ã‚‹å ´åˆã®ã‚¨ãƒ©ãƒ¼å›é¿
                    link.scroll_into_view_if_needed()
                    link.click(force=True)
                    page_list.wait_for_timeout(1500) # è©³ç´°ãƒ­ãƒ¼ãƒ‰å¾…ã¡

                    # --- å³ãƒ‘ãƒãƒ«ã®è§£æ ---
                    # job_offersã‚’å«ã‚€ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                    job_links = page_list.locator("a[href*='/job_offers/']").all()
                    
                    if not job_links:
                        # æ±‚äººãƒªãƒ³ã‚¯ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ­ã‚°ã‚’ã†ã‚‹ã•ãã—ãªã„ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆå¯ï¼‰
                        # print(f"  [{i}] â© ã‚¹ã‚­ãƒƒãƒ—: æ±‚äººãƒªãƒ³ã‚¯ãªã— (ä»¶å: {title_text})")
                        continue

                    job_url_suffix = job_links[0].get_attribute("href")
                    job_url = f"https://paiza.jp{job_url_suffix}" if not job_url_suffix.startswith("http") else job_url_suffix
                    
                    # IDæŠ½å‡º
                    match = re.search(r'/job_offers/(\d+)', job_url)
                    job_id = match.group(1) if match else job_url

                    # DBãƒã‚§ãƒƒã‚¯ (é«˜é€ŸåŒ–ã®è¦)
                    if db.is_processed(job_id):
                        print(f"  [{i}] â™»ï¸ å‡¦ç†æ¸ˆã¿: {title_text}... (ID:{job_id})")
                        continue

                    # --- [ã‚¿ãƒ–B] è§£æå®Ÿè¡Œ ---
                    print(f"  [{i}] ğŸ†• è§£æé–‹å§‹: {title_text}... (ID:{job_id})")
                    
                    page_worker.goto(job_url)
                    page_worker.wait_for_load_state("domcontentloaded")
                    job_text = page_worker.locator("body").inner_text()[:6000]

                    print("    ğŸ§  AIåˆ†æä¸­...")
                    result = analyze_job_description(job_text, MY_PROFILE)
                    score = result.get('score', 0)
                    print(f"    ğŸ¯ ã‚¹ã‚³ã‚¢: {score}ç‚¹")

                    if score >= SCORE_THRESHOLD:
                        print("    ğŸ”” Discordé€šçŸ¥é€ä¿¡")
                        dummy_scout = {'subject': f"ã€Webè§£æã€‘{title_text}", 'url': job_url}
                        send_discord_notify(dummy_scout, result)
                    else:
                        print("    ğŸ—‘ï¸ ã‚¹ã‚³ã‚¢ä¸è¶³")

                    db.save_job_record(job_id, job_url, score)
                    processed_count += 1
                    time.sleep(2) # ãƒãƒŠãƒ¼å¾…æ©Ÿ

                except Exception as e:
                    # å€‹åˆ¥ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦æ¬¡ã¸
                    continue

            print(f"âœ… å®Œäº†: {processed_count} ä»¶ã®æ–°è¦ã‚¹ã‚«ã‚¦ãƒˆã‚’å‡¦ç†ã—ã¾ã—ãŸã€‚")

        except Exception as e:
            print(f"âŒ å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            browser.close()

if __name__ == "__main__":
    run_crawler()
