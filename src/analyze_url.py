import re
from playwright.sync_api import sync_playwright

AUTH_FILE = "auth.json"

def clean_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ç”¨"""
    if not text: return ""
    text = text.replace("\n", " ").replace("ã€€", " ")
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'\s+', '', text)
    return text

def extract_smart_keyword(subject):
    """ä»¶åã‹ã‚‰æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¼šç¤¾åå„ªå…ˆï¼‰ã‚’æŠ½å‡º"""
    # 1. è‹±èªã®ä¼šç¤¾å (ä¾‹: CARTA HOLDINGS)
    english_name = re.search(r'[a-zA-Z][a-zA-Z\s\.]+(?:Inc\.|Corp\.|Holdings|Group)?', subject, re.IGNORECASE)
    if english_name and len(english_name.group(0).strip()) > 3:
        return clean_text(english_name.group(0))

    # 2. æ ªå¼ä¼šç¤¾ã€‡ã€‡
    jp_company = re.search(r'æ ªå¼ä¼šç¤¾\s*(\S+)', subject)
    if jp_company:
        return clean_text(jp_company.group(1))

    # 3. ãªã‘ã‚Œã°ä»¶åã®å†’é ­10æ–‡å­—
    return clean_text(subject)[:10]

def fetch_job_text(target_url, target_subject=None):
    """
    URLã‚’é–‹ãã€ä»¶åã«ä¸€è‡´ã™ã‚‹ã‚¹ã‚«ã‚¦ãƒˆã‚’æ¢ã—ã¦æ±‚äººè©³ç´°ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—ã‚’è¿”ã™
    """
    keyword = extract_smart_keyword(target_subject) if target_subject else ""
    print(f"ğŸš€ è§£æé–‹å§‹: {keyword}")

    extracted_text = ""

    with sync_playwright() as p:
        # æœ¬ç•ªé‹ç”¨æ™‚ã¯ headless=True ã«ã—ã¦ã‚‚OKã§ã™ãŒã€å‹•ããŒè¦‹ãŸã„å ´åˆã¯Falseã§
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(storage_state=AUTH_FILE)
        page = context.new_page()

        try:
            page.goto(target_url)
            page.wait_for_load_state("domcontentloaded")
            
            job_url_found = None
            
            if "messages" in page.url:
                print(" |__ ğŸ•µï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¸€è¦§ã‹ã‚‰å¯¾è±¡ã‚’æ¤œç´¢ä¸­...")
                
                sidebar_selector = ".p-messages-scout-messages"
                try:
                    sidebar = page.locator(sidebar_selector)
                    sidebar.wait_for(state="visible", timeout=5000)
                    sidebar.hover() # ãƒ•ã‚©ãƒ¼ã‚«ã‚¹
                except:
                    print(" |__ âŒ ã‚µã‚¤ãƒ‰ãƒãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return ""

                target_message_link = None
                found = False

                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æ¢ç´¢ãƒ«ãƒ¼ãƒ—
                for i in range(10):
                    current_links = sidebar.locator("a").all()
                    for link in current_links:
                        text = clean_text(link.inner_text())
                        if keyword and keyword.lower() in text.lower():
                            print(f" |__ âœ¨ ç™ºè¦‹: {text[:20]}...")
                            target_message_link = link
                            found = True
                            break
                    if found: break
                    
                    # ç‰©ç†ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                    page.mouse.wheel(0, 2000)
                    page.wait_for_timeout(1000)

                if target_message_link:
                    # ã‚¯ãƒªãƒƒã‚¯å‡¦ç†
                    try:
                        target_message_link.click(force=True)
                        page.wait_for_timeout(2500) # è©³ç´°ãƒ­ãƒ¼ãƒ‰å¾…ã¡
                    except Exception as e:
                        print(f" |__ âš ï¸ ã‚¯ãƒªãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                        return ""

                    # è©³ç´°ãƒ‘ãƒãƒ«ã‹ã‚‰æ±‚äººIDå–å¾—
                    detail_links = page.locator("a[href*='/job_offers/']").all()
                    if detail_links:
                        job_url_found = detail_links[0].get_attribute("href")
                        print(f" |__ âœ… æ±‚äººãƒšãƒ¼ã‚¸ç‰¹å®š: {job_url_found}")
                    else:
                        print(" |__ âŒ è©³ç´°ãƒ‘ãƒãƒ«å†…ã«æ±‚äººãƒªãƒ³ã‚¯ãªã—")
                        return ""
                else:
                    print(" |__ âŒ ãƒªã‚¹ãƒˆå†…ã«è©²å½“ãªã—(æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§)")
                    return ""

            # æ±‚äººè©³ç´°ãƒšãƒ¼ã‚¸ã¸ç§»å‹•
            if job_url_found:
                if not job_url_found.startswith("http"):
                    job_url_found = "https://paiza.jp" + job_url_found
                    
                page.goto(job_url_found)
                page.wait_for_load_state("domcontentloaded")
                page.wait_for_timeout(2000)
                extracted_text = page.locator("body").inner_text()

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            browser.close()

    return extracted_text[:5000]
