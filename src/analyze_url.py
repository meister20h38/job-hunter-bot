# src/analyze_url.py (æ­£è¦è¡¨ç¾ã®å¼·åŒ–ç‰ˆ)
import re
from playwright.sync_api import sync_playwright

AUTH_FILE = "auth.json"

def clean_text(text):
    if not text: return ""
    text = text.replace("\n", " ").replace("ã€€", " ")
    text = re.sub(r'ã€.*?ã€‘', '', text)
    text = re.sub(r'\s+', '', text)
    return text

def extract_smart_keyword(subject, body_text=""):
    """
    æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒã‚¤ã‚ºé™¤å»å¼·åŒ–ç‰ˆï¼‰
    """
    # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆã“ã‚Œã‚‰ãŒã€Œç¤¾åã€ã¨ã—ã¦æŠ½å‡ºã•ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰
    IGNORE_WORDS = ["days", "scout", "paiza", "offer", "interview", "engineer", "recruitment"]

    # 1. ä»¶åã‹ã‚‰è‹±èªç¤¾å (å¤§æ–‡å­—å§‹ã¾ã‚Šã€3æ–‡å­—ä»¥ä¸Š)
    # ä¿®æ­£: ãƒã‚¤ãƒ•ãƒ³å˜ä½“ã‚„å°æ–‡å­—ã®ã¿ã®å˜èªã‚’é™¤å¤–
    english_name_matches = re.finditer(r'\b[A-Z][a-zA-Z\s\.]+(?:Inc\.|Corp\.|Holdings|Group)?', subject)
    for match in english_name_matches:
        word = clean_text(match.group(0))
        if len(word) > 3 and word.lower() not in IGNORE_WORDS:
            return word

    # 2. ä»¶åã‹ã‚‰æ ªå¼ä¼šç¤¾ã€‡ã€‡
    jp_company = re.search(r'æ ªå¼ä¼šç¤¾\s*([^\sï¼/!ï¼]+)', subject)
    if jp_company:
        return clean_text(jp_company.group(1))

    # --- æœ¬æ–‡ã‚µãƒ¼ãƒ ---
    if body_text:
        # 3. æœ¬æ–‡å†’é ­ã®è‹±èªç¤¾å (CARTA HOLDINGSãªã©)
        body_eng = re.search(r'^([A-Z][a-zA-Z\s\.]+)(?:ï¼|/)', body_text.strip(), re.MULTILINE)
        if body_eng:
            word = clean_text(body_eng.group(1))
            if len(word) > 3 and word.lower() not in IGNORE_WORDS:
                return word

        # 4. æœ¬æ–‡ã‹ã‚‰æ ªå¼ä¼šç¤¾ã€‡ã€‡
        body_jp = re.search(r'æ ªå¼ä¼šç¤¾\s*([^\sï¼/!ï¼\-\=]+)', body_text)
        if body_jp:
            return clean_text(body_jp.group(1))
            
        # 5. ã€ç¤¾åã€‘ã€‡ã€‡
        footer_company = re.search(r'ã€ç¤¾åã€‘\s*([^\s]+)', body_text)
        if footer_company:
            return clean_text(footer_company.group(1))

    # 6. æœ€çµ‚æ‰‹æ®µ: ä»¶åã®å†’é ­10æ–‡å­—ï¼ˆãŸã ã—è¨˜å·ã¯å‰Šé™¤ï¼‰
    # ãƒã‚¤ãƒ•ãƒ³ã‚„è¨˜å·ãŒé€£ç¶šã—ã¦ã„ã‚‹å ´åˆã¯é™¤å»ã™ã‚‹
    cleaned_subject = clean_text(subject)
    cleaned_subject = re.sub(r'[\-\=]{2,}', '', cleaned_subject) # --ã‚„==ã‚’å‰Šé™¤
    return cleaned_subject[:10]

def fetch_job_text(target_url, target_subject=None, body_preview=None):
    # é–¢æ•°ã®ä¸­èº«ã¯å‰å›ã®ã€Œæ±ºå®šç‰ˆï¼ˆbody_previewå¯¾å¿œï¼‰ã€ã®ã¾ã¾å¤‰æ›´ãªã—
    # ãŸã ã— extract_smart_keyword ã¯ä¸Šè¨˜ã®ã‚‚ã®ã‚’ä½¿ã†
    
    keyword = extract_smart_keyword(target_subject, body_preview) if target_subject else ""
    print(f"ğŸš€ è§£æé–‹å§‹: {keyword}")

    extracted_text = ""

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context(storage_state=AUTH_FILE)
        page = context.new_page()

        try:
            page.goto(target_url)
            page.wait_for_load_state("domcontentloaded")
            
            job_url_found = None
            
            if "messages" in page.url:
                print(" |__ ğŸ•µï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¸€è¦§ã‹ã‚‰å¯¾è±¡ã‚’æ¤œç´¢ä¸­...")
                
                sidebar_selector = ".p-messages-scout-messages"
                try:
                    sidebar = page.locator(sidebar_selector)
                    sidebar.wait_for(state="visible", timeout=5000)
                    sidebar.hover()
                except:
                    print(" |__ âŒ ã‚µã‚¤ãƒ‰ãƒãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return ""

                target_message_link = None
                found = False

                for i in range(10):
                    current_links = sidebar.locator("a").all()
                    for link in current_links:
                        text = clean_text(link.inner_text())
                        if keyword and keyword.lower() in text.lower():
                            print(f" |__ âœ¨ ç™ºè¦‹: {text[:20]}...")
                            target_message_link = link
                            found = True
                            break
                    if found: break
                    
                    page.mouse.wheel(0, 2000)
                    page.wait_for_timeout(1000)

                if target_message_link:
                    try:
                        target_message_link.click(force=True)
                        page.wait_for_timeout(2500)
                    except Exception as e:
                        print(f" |__ âš ï¸ ã‚¯ãƒªãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                        return ""

                    detail_links = page.locator("a[href*='/job_offers/']").all()
                    if detail_links:
                        job_url_found = detail_links[0].get_attribute("href")
                        print(f" |__ âœ… æ±‚äººãƒšãƒ¼ã‚¸ç‰¹å®š: {job_url_found}")
                    else:
                        print(" |__ âŒ è©³ç´°ãƒ‘ãƒãƒ«å†…ã«æ±‚äººãƒªãƒ³ã‚¯ãªã—")
                        return ""
                else:
                    print(" |__ âŒ ãƒªã‚¹ãƒˆå†…ã«è©²å½“ãªã—(æœŸé™åˆ‡ã‚Œã®å¯èƒ½æ€§)")
                    return ""

            if job_url_found:
                if not job_url_found.startswith("http"):
                    job_url_found = "https://paiza.jp" + job_url_found
                    
                page.goto(job_url_found)
                page.wait_for_load_state("domcontentloaded")
                page.wait_for_timeout(2000)
                extracted_text = page.locator("body").inner_text()

        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            browser.close()

    return extracted_text[:5000]
